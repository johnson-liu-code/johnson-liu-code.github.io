<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Johnson Liu — AI Project Portfolio</title>
    <link rel="stylesheet" href="../../css/style.css">
</head>

<body>

    <header>
        <div class="hero">
            <img src="../../home_images/ising_loop.gif" alt="ising_evolution_animation">
            <div class="hero-text">
                <h1>Johnson Liu</h1>
                <p>Projects Portfolio</p>
            </div>
        </div>
    </header>

    <div class="content">
        <nav>
        <ul>
            <li><a href="../../home.html">Home</a></li>
            <li><a href="llm_jailbreak.html">LLM Jailbreak</a></li>
            <li><a href="../sentiment_analysis/sentiment.html">Sentiment Analysis</a></li>
            <li><a href="../dominion_ai/dominion.html">Dominion AI</a></li>
            <li><a href="../inquisitorNet/inquisitor.html">InquisitorNet</a></li>
            <li><a href="../civicsim/civicsim.html">CivicSim</a></li>
            <li><a href="../scp/scp.html">SCP Wiki Entries</a></li>
        </ul>
        </nav>
        <main>
            <section class="project-header">
                <div class="project-header__text">
                    <h2>
                        Large Language Model Jailbreak Stress Testing Dashboard<br>
                        <span class="subtitle">— An interactive dashboard for stress testing LLMs</span>
                    </h2>

                    <br>

                    <hr>

                    <p style="text-align: center">——— Work in Progress ———</p>
                    <!-- <p>
                        <a href="https://github.com/johnson-liu-code/InquisitorNet" target="_blank" rel="noopener">
                            View the code on GitHub ...
                        </a>
                    </p> -->
                    <p>
                        This mini jailbreak stress test dashboard is an app for stress testing 
                        large language models (LLMs) with custom and premade prompts gathered 
                        from open-source projects.
                        
                        This tool allows for interactive monitoring of attacks on various models 
                        and the visualization of these models' performance in defending against 
                        such attacks.
                        
                        The app provides a real-time display in the form of a prompt refusal 
                        table and graphs that lets the user quickly spot successful jailbreaks 
                        from the provided prompts along with jailbreaking trends that emerge from 
                        the use of a variety of prompts across multiple LLMs.
                    </p>
                </div>
                <div class="project-header__img">
                    <img src="llm_jailbreak.png" alt="llm Jailbreak image">
                    <figcaption style="text-align: center">
                        <i>Figure generated with GPT-5.</i>
                    </figcaption>
                </div>
            </section>

            <section>
                <h3>Project Overview</h3>
                <b>Goal</b>
                    <hr>
                    <ol>
                        <li>
                            Build transparent, reproducible evaluations of LLM capability and 
                            safety (with a focus on jailbreak susceptibility).
                        </li>
                        <li>
                            Create practical tools and write-ups that help students, researchers, 
                            and engineers reason about model behavior.
                        </li>
                        <li>
                            Contribute open resources (code, datasets, and reports) that advance 
                            LLM alignment and safety.
                        </li>
                    </ol>
                
                <b>Current progress</b>
                    <hr>
                    <ol>
                        <li>
                            Implemented a Streamlit-based Jailbreak Stress-Test Dashboard with 
                            multi-model runs, batch prompting, logging, and results tables.
                        </li>
                        <li>
                            Organized a library of attack prompts and began benchmarking 
                            success/refusal rates across commercial and open-weight models.
                        </li>
                        <li>
                            Drafted portfolio copy and literature notes covering jailbreak 
                            taxonomy, defenses, and evaluation methods.
                        </li>
                    </ol>
                
                <b>Future plans</b>
                    <hr>
                    <ol>
                        <li>
                            Add automated red-teaming (model-generated attacks), attack families, 
                            and per-model safety scorecards to the dashboard.
                        </li>
                        <li>
                            Submit a prompt response dataset with evaluation scripts and baseline 
                            metrics to JailbreakBench.
                        </li>
                        <li>
                            Test defenses (prompt hardening, safety-tuned adapters, retrieval 
                            guardrails) and publish a short report with findings.
                        </li>
                    </ol>

                </section>

                <section>
                <h3>Background on LLMs</h3>
                <hr>
                <p>
                    Large language models are transformer-based neural networks with very large parameter 
                    counts, trained with self-supervised learning on massive text corpora.

                    Through self-attention mechanisms, they learn rich syntactic and semantic structure, 
                    capturing which tokens matter within the context of a given piece of text and by how much.

                    When prompted, they generate coherent text and can handle tasks such as engaging in dialogue, 
                    question answering, summarization, classification, and translation.

                    Because they're trained on both natural and constructed languages (e.g., programming languages), 
                    many models can also generate and edit code.
                    <br>
                    <br>
                    At their core, LLMs are probabilistic next-token predictors: given context, they model a 
                    distribution over possible continuations for the text and decode from it.
                    
                    This simple objective, scaled to large data and computation, yields broad generalization.

                    The base models can then be adapted, via prompting, fine-tuning, information and data 
                    retrieval, or tool integration, to specialize for particular tasks with greater reliability.
                    <br>
                    <br>
                    However, these same generative mechanisms create opportunities and corridors for attack.

                    Carefully designed prompts can exploit model behavior and elicit outputs that violate safety 
                    policies, a practice known as "jailbreaking."

                    Understanding how such attacks steer LLM text generation is essential to reducing misuse.

                    Despite ongoing progress, both leading commercial systems (e.g., GPT, Claude) and popular 
                    open-weight models (e.g., Llama, Mistral) remain susceptible, motivating continued research 
                    into robust alignment and defenses.
                </p>

                <br>
                <br>

                <p>
                    <a href="../../home.html">
                        <h3>Return to Home Page</h3>
                    </a>
                </p>

            </section>
        </main>
    </div>

    <footer>
        <p>
            <span id="last-updated">Last updated </span> • Site hosted with GitHub Pages
        </p>
    </footer>

    <script src="/js/last-updated.js"></script>
    <script>

    const repoRelativePath = window.location.pathname.replace(/^\//, '');

    updateLastUpdated({
        elementId: 'last-updated',
        owner: 'johnson-liu-code',
        repo: 'johnson-liu-code.github.io',
        filePath: repoRelativePath,
        fallbackToNow: true
    });
    </script>
        
    <script>
    document.querySelectorAll("nav a").forEach(link => {
        // Compare just the pathname (so query‑strings or domain don’t matter)
        if (link.pathname === window.location.pathname) {
        link.classList.add("active");
        }
    });
    </script>

</body>
</html>